{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Collaboration and Competition - Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Overview Of The Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this project our goal is to train two agents to control racket to bounce a ball over the net. A reward of 0.1 is provided for every time the agent hits the ball over the net thus enabling the agent to prioritize hitting the ball over the net else it receives a reward of -0.01.\n",
    "\n",
    "> We have 8 possible states corresponding to position and velocity of the ball and racket.Each agent receives its own, local observation. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping.The task is episodic, and in order to solve the environment, your agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Algorithm Explanation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Agent Reinforcement Learning(MADDPG Algorithm):\n",
    "> This environment is quite interesting compared to single agent environments. It requires the training of two separate agents, and the agents need to collaborate under certain situations (like don’t let the ball hit the ground) and compete under other situations (like gather as many points as possible). Just doing a simple extension of single agent RL by independently training the two agents does not work very well because the agents are independently updating their policies as learning progresses. And this causes the environment to appear non-stationary from the viewpoint of any one agent. While we can have non-stationary Markov processes, the convergence guarantees offered by many RL algorithms such as Q-learning requires stationary environments. While there are many different RL algorithms for multi-agent settings, for this project I chose to use the Multi Agent Deep Deterministic Policy Gradient (MADDPG) algorithm\n",
    "\n",
    "> In MADDPG, each agent’s critic is trained using the observations and actions from all the agents, whereas each agent’s actor is trained using just its own observations. This allows the agents to be effectively trained without requiring other agents’ observations during inference (because the actor is only dependent on its own observations).\n",
    "\n",
    "<img src='M1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deep Deterministic Policy Gradients :\n",
    "> DDPG uses four neural networks: a Q network, a deterministic policy network, a target Q network, and a target policy network.\n",
    "\n",
    "<img src='1.png'>\n",
    "\n",
    "> The Q network and policy network is very much like simple Advantage Actor-Critic, but in DDPG, the Actor directly maps states to actions (the output of the network directly the output) instead of outputting the probability distribution across a discrete action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The target networks are time-delayed copies of their original networks that slowly track the learned networks. Using these target value networks greatly improve stability in learning. Here’s why: In methods that do not use target networks, the update equations of the network are interdependent on the values calculated by the network itself, which makes it prone to divergence.\n",
    "\n",
    "<img src='2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DDPG Algorithm:\n",
    "\n",
    "<img src='3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replay Buffer:\n",
    "> As used in Deep Q learning (and many other RL algorithms), DDPG also uses a replay buffer to sample experience to update neural network parameters. During each trajectory roll-out, we save all the experience tuples (state, action, reward, next_state) and store them in a finite-sized cache — a “replay buffer.” Then, we sample random mini-batches of experience from the replay buffer when we update the value and policy networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Actor (Policy) & Critic (Value) Network:\n",
    "> The value network is updated similarly as is done in Q-learning. The updated Q value is obtained by the Bellman equation:\n",
    "\n",
    "<img src='4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> However, in DDPG, the next-state Q values are calculated with the target value network and target policy network. Then, we minimize the mean-squared loss between the updated Q value and the original Q value:\n",
    "<img src='5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For the policy function, our objective is to maximize the expected return:\n",
    "<img src='6.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To calculate the policy loss, we take the derivative of the objective function with respect to the policy parameter. Keep in mind that the actor (policy) function is differentiable, so we have to apply the chain rule.\n",
    "<img src='7.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> But since we are updating the policy in an off-policy way with batches of experience, we take the mean of the sum of gradients calculated from the mini-batch:\n",
    "<img src='8.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters Used :\n",
    "<p>\n",
    "BUFFER_SIZE = int(1e6) \n",
    "    \n",
    "EPSILON = 1.0 \n",
    "\n",
    "EPSILON_DECAY = 1e-6 \n",
    "\n",
    "WEIGHT_DECAY = 0     \n",
    "\n",
    "BATCH_SIZE = 256  \n",
    "\n",
    "OU_SIGMA = 0.1\n",
    "\n",
    "OU_THETA = 0.15\n",
    "\n",
    "GAMMA = 0.99      \n",
    "\n",
    "TAU = 2e-3         \n",
    "\n",
    "LR_ACTOR = 1e-3     \n",
    "\n",
    "LR_CRITIC = 1e-3     \n",
    "\n",
    "LEARN_EVERY = 1       \n",
    "\n",
    "LEARN_NUM = 10         \n",
    "\n",
    "GRAD_CLIPPING = 1.0 \n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Architecture:\n",
    "> Here we have two different networks for actor and critic. Below i have mentioned the architectures of the both networks.\n",
    "\n",
    "                Actor:\n",
    ">                   fc1(399 units) -- batch_normalization -- fc2(299 units) -- fc3(2 units)\n",
    "\n",
    "              Critic:\n",
    ">                   fc1(399 units) -- batch_normalization -- fc2(299 units + 2 units) -- fc3(1 units)\n",
    "\n",
    "> Here we have used relu as the activation function and adam as the optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:\n",
    "> It took 337 episodes to solve the environment.\t with Average score: 0.505\n",
    "<img src='9.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideas For Further Improvement:\n",
    "> Algorithms such as MAPPO can also be considered for these kind of problems.\n",
    "\n",
    "> Experiment with different values for hyperparameters such as fc1 units, fc2 units, batch_size etc.\n",
    "\n",
    "> Adding a few additional layers to the actor and critic networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
